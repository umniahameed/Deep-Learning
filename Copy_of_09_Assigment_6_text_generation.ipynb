{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umniahameed/Deep-Learning/blob/main/Copy_of_09_Assigment_6_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Split into training (80%) and validation (20%)."
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "\n",
        "# Download the book text\n",
        "response = requests.get(url)\n",
        "book_text = response.text\n",
        "\n",
        "\n",
        "# Split the text into training (80%) and validation (20%)\n",
        "text_words = book_text.split()\n",
        "\n",
        "# Calculate the split index for 80-20\n",
        "split_index = int(0.8 * len(text_words))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_text = text_words[:split_index]\n",
        "valid_text = text_words[split_index:]\n",
        "\n",
        "# Convert the list of words back into text\n",
        "train_text = \" \".join(train_text)\n",
        "valid_text = \" \".join(valid_text)\n",
        "\n",
        "# Display the lengths of the training and validation sets\n",
        "print(f\"Training set length: {len(train_text)} words\")\n",
        "print(f\"Validation set length: {len(valid_text)} words\")\n"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14347b4-d1f0-45f1-dcfd-1804920faeda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 576961 words\n",
            "Validation set length: 141601 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Convert the text to lowercase\n",
        "book_text = book_text.lower()\n",
        "\n",
        "# Remove punctuation (except basic sentence delimiters)\n",
        "book_text = re.sub(r'[^a-z\\s.,!?\\'\"]', '', book_text)\n",
        "\n",
        "# Tokenize the text by words\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the text (this builds the vocabulary)\n",
        "tokenizer.fit_on_texts([book_text])\n",
        "\n",
        "# Convert the text into sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences([book_text])[0]\n",
        "\n",
        "# Build the vocabulary\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 because the index starts from 1, not 0\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626411a9-8849-4bd1-a961-b5a921dfea3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 8581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # vocabulary size (includes 0 for padding)\n",
        "sequence_length = 50  # Length of input sequences\n",
        "\n",
        "# Define the embedding layer\n",
        "embedding_layer = Embedding(input_dim=vocab_size,    # size of the vocabulary\n",
        "                            output_dim=128,        # embedding dimension\n",
        "                            input_length=sequence_length)  # input sequence length\n"
      ],
      "metadata": {
        "id": "OXCK40l6MRld"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YoEM682Eiq36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)  # Add the embedding layer\n",
        "model.add(LSTM(256, return_sequences=False))  # Add the LSTM layer (256 units)\n",
        "model.add(Dense(vocab_size, activation='softmax'))  # Output layer with softmax activation for word prediction\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "linweGaUMg0T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "26cd4724-a903-46eb-eb88-b9ee35441905"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible. If you have higher value (which is possible) try to draw conclusions, why doesn't it decrease to a lower value."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create input-output pairs for training\n",
        "input_sequences = []\n",
        "output_words = []\n",
        "\n",
        "for i in range(sequence_length, len(sequences)):\n",
        "    input_sequences.append(sequences[i-sequence_length:i])\n",
        "    output_words.append(sequences[i])\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "output_words = np.array(output_words)\n",
        "\n",
        "# Display shapes of the input and output\n",
        "print(f\"Input sequences shape: {input_sequences.shape}\")\n",
        "print(f\"Output words shape: {output_words.shape}\")\n"
      ],
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee9d262-f38f-49e2-c1cd-00d143128cf9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequences shape: (133398, 50)\n",
            "Output words shape: (133398,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(input_sequences, output_words, epochs=5, batch_size=64, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1t69OiCi0a_",
        "outputId": "d7ad81c8-3e83-40cd-b6d6-c29ec63b4557"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 398ms/step - accuracy: 0.0594 - loss: 6.6482 - val_accuracy: 0.1101 - val_loss: 5.7679\n",
            "Epoch 2/5\n",
            "\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 394ms/step - accuracy: 0.1235 - loss: 5.5514 - val_accuracy: 0.1316 - val_loss: 5.5079\n",
            "Epoch 3/5\n",
            "\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 393ms/step - accuracy: 0.1506 - loss: 5.1082 - val_accuracy: 0.1437 - val_loss: 5.4289\n",
            "Epoch 4/5\n",
            "\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 396ms/step - accuracy: 0.1699 - loss: 4.7400 - val_accuracy: 0.1447 - val_loss: 5.4366\n",
            "Epoch 5/5\n",
            "\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 393ms/step - accuracy: 0.1880 - loss: 4.4039 - val_accuracy: 0.1441 - val_loss: 5.5137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, length=50):\n",
        "    result = seed_text\n",
        "    for _ in range(length):\n",
        "        # Convert seed text to sequence of word IDs\n",
        "        token_list = tokenizer.texts_to_sequences([result])[0]\n",
        "\n",
        "        # Keep only the last `sequence_length` tokens for prediction\n",
        "        token_list = token_list[-sequence_length:]\n",
        "\n",
        "        # Pad sequence if it's shorter than required length\n",
        "        padded = np.pad(token_list, (sequence_length - len(token_list), 0), 'constant')\n",
        "        padded = padded.reshape(1, sequence_length)\n",
        "\n",
        "        # Predict the next wrd\n",
        "        predicted = np.argmax(model.predict(padded, verbose=0), axis=-1)[0]\n",
        "\n",
        "        # Convert predicted ID back to word\n",
        "        output_word = tokenizer.index_word.get(predicted, '')\n",
        "        if not output_word:\n",
        "            break\n",
        "        result += ' ' + output_word\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate two samples with dfferent seed phrases\n",
        "seed1 = \"love is\"\n",
        "seed2 = \"time will\"\n",
        "\n",
        "sample1 = generate_text(model, tokenizer, seed1, length=50)\n",
        "sample2 = generate_text(model, tokenizer, seed2, length=50)\n",
        "\n",
        "print(\"Sample 1:\")\n",
        "print(sample1)\n",
        "print(\"\\nSample 2:\")\n",
        "print(sample2)\n",
        "\n"
      ],
      "metadata": {
        "id": "n5CpdqF9MoPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9dc3cb-156d-4c79-cf85-5825b8719a3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "love is not to be in the same and the day and the whole party were over the whole party and the whole party were over the whole party and the whole party were over the whole party and the whole party were over the whole party and the whole party were\n",
            "\n",
            "Sample 2:\n",
            "time will be a great deal of the same time to the same and the day and the whole party were over the whole party and the whole party were over the whole party and the whole party were over the whole party and the whole party were over the whole party\n"
          ]
        }
      ]
    }
  ]
}